{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f380c37",
   "metadata": {},
   "source": [
    "# Task Management Project Analysis\n",
    "\n",
    "This notebook contains the analysis of task management data from Jira, including data loading, exploration, cleaning, and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a69a32",
   "metadata": {},
   "source": [
    "## Week 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd126e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957baf0c",
   "metadata": {},
   "source": [
    "### 1. Loading and Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('jira_dataset.csv')\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First few rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types of columns:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4242d",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aaa319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task priority distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='priority')\n",
    "plt.title('Distribution of Task Priorities')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1131595",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc487c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Handle missing values\n",
    "df = df.fillna({'description': '', 'priority': 'Medium'})\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(\"Dataset shape after cleaning:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8ae35",
   "metadata": {},
   "source": [
    "### 4. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad455fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to description column\n",
    "df['clean_description'] = df['project_description'].apply(preprocess_text)\n",
    "\n",
    "# Display sample of original and cleaned descriptions\n",
    "print(\"Sample of original and cleaned descriptions:\")\n",
    "display(pd.DataFrame({\n",
    "    'Original': df['project_description'].head(),\n",
    "    'Cleaned': df['clean_description'].head()\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb795f1",
   "metadata": {},
   "source": [
    "## Week 2: Feature Engineering and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28345422",
   "metadata": {},
   "source": [
    "### 6. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "tfidf_features = tfidf.fit_transform(df['clean_description'])\n",
    "\n",
    "# Word2Vec Vectorization\n",
    "sentences = [text.split() for text in df['clean_description']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get document vectors using Word2Vec\n",
    "def get_document_vector(text):\n",
    "    words = text.split()\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    return np.zeros(100)\n",
    "\n",
    "word2vec_features = np.array([get_document_vector(text) for text in df['clean_description']])\n",
    "\n",
    "print(\"TF-IDF features shape:\", tfidf_features.shape)\n",
    "print(\"Word2Vec features shape:\", word2vec_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e2e8b",
   "metadata": {},
   "source": [
    "### 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199aec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['priority'])\n",
    "\n",
    "# Split data for TF-IDF features\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
    "    tfidf_features, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split data for Word2Vec features\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    word2vec_features, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64fcfe",
   "metadata": {},
   "source": [
    "### 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'\n",
    "{model_name} Results:')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred, average='weighted'):.3f}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred, average='weighted'):.3f}')\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate both models\n",
    "evaluate_model(nb_model, X_test_tfidf, y_test, 'Naive Bayes')\n",
    "evaluate_model(svm_model, X_test_tfidf, y_test, 'SVM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
